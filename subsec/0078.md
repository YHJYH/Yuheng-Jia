---
layout: default
---
## Supplementary notes for COMP0078 Supervised Learning
- [Lecture 01 LR, k-NN, Bayes estimator, Bias-Variance trade-off, NFL theorem, cross validation](https://github.com/YHJYH/Yuheng-Jia/blob/main/subsec/0078.md#lecture-01-lr-k-nn-bayes-estimator-bias-variance-trade-off-nfl-theorem-cross-validation)
- [Lecture 02]()

### Lecture 01 LR, k-NN, Bayes estimator, Bias-Variance trade-off, NFL theorem, cross validation
- Linear regression
    - linear decision boundary 
    - *Xw = Y*
    - minimise square error (MSE), or ERM using square error (emipirical risk minimization)
    - deterministic solution: $w = (X^{T}X)^{-1}X^{T}Y$ by solving normal equation
    - adding a bias term in *w* by inserting a row of 1s in *X*
    - Bayes estimator for square loss: $f^{\*}(x) = E[y|x]$. Lec01 P40.

- k-NN k-nearest neighbour
    - non-linear decision boundary
    - classification: f(x) = majority vote of $I_{x} = {i: x_{i} \in N(x;k)}$ N(x;k) is the set of k nearest training inputs
    - regression: $f(x) = \frac{1}{k} \sum_{i \in I_{x}}y_{i}$ ("*local mean*")
    - small k: overfitting, irregular decision boundary; large k: underfitting, smoothing decision boundary.
    - Bayes rule/estimator/clasifier for the K-class classification problem: $\arg_{k}\max P(y=k|x)$; corresponding Bayes error rate/risk: $E_{x}[1-\max_{k}P(y=k|x)]$. 
    - [Asymptotic optimality of k-NN](https://www.cs.cmu.edu/~aarti/Class/10701/recitation/knn_asymp.pdf): the asymptotic expected risk of 1NN is no more than two times the Bayes risk. Lec01 P50.
    - [kNN from a Bayesian viewpoint](https://stats.stackexchange.com/a/157509): $P(y=k|x)=K_{k}/K$. Lec01 P49.

- bias-variance trade-off
    - expected square error = Bayes error + bias^2 + variance. Lec01 P43. [Decomposition](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html)
    - [solution](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html#:~:text=.-,Regime%201%20(High%20Variance),-In%20the%20first) to high bias/variance.

- LR vs. k-NN (Lec01 P55)
    - linear & non-linear
    - parametric & non-parametric
    - global & local
    - stable & unstable
    - large bias & low bias
    - k-NN sensitive to feature dimension *d*

- No-free-lunch theorem: see [0175](https://github.com/YHJYH/Yuheng-Jia/blob/main/subsec/0175.md)

- cross-validation
    - k-fold cross-validation: split data into k parts
    - if k = n = sample size: leave-one-out (LOO)
    - cross-validation error: average of the errors of k validation sets
    - smaller k: less expensive but worse performance. Lec01 P66.

### Lecture 02 
- General knowledge
    - vector space $\in$ inner product space
    - norm/metric, see [0175](https://github.com/YHJYH/Yuheng-Jia/blob/main/subsec/0175.md)
    - a symmetric matrix M is PSD iff $x^{T}Mx \geq 0$
    - convexity Lec02 P9

- ridge regression / Tikhonov regularization
    - add regularized term to empirical square error $\lambda ||w||^{2}\_{2}$ 
    - deterministic solution by solving modified normal equation $w = (X^{T}X + \lambda I_{n})^{-1} X^{T}Y$, n: # sampels
    - $\alpha = (XX^{T} + \lambda I_{m})^{-1}Y$, m: # attributes. $w = \sum_{i=1}^{m} \alpha_{i}x_{i}$
    - primal form: $f(x) = w^{T}x$, dual form: $f(x) = \sum_{i=1}^{m} \alpha_{i}x_{i}^{T}x$
- kernel methods




[back](../)
