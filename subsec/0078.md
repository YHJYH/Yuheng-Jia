---
layout: default
---
## Supplementary notes for COMP0078 Supervised Learning
- Linear regression
    - linear decision boundary 
    - *Xw = Y*
    - minimise square error (MSE), or ERM using square error (emipirical risk minimization)
    - deterministic solution: $w = (X^{T}X)^{-1}X^{T}Y$
    - adding a bias term in *w* by inserting a row of 1s in *X*

- k-NN k-nearest neighbour
    - non-linear decision boundary
    - classification: f(x) = majority vote of $I_{x} = {i: x_{i} \in N(x;k)}$ N(x;k) is the set of k nearest training inputs;
    - regression: $f(x) = \frac{1}{k} \sum_{i \in I_{x}}y_{i}$ ("*local mean*")

- kernel methods




[back](../)
