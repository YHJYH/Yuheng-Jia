---
layout: default
---
## Supplementary notes for COMP0078 Supervised Learning
- Linear regression
    - linear decision boundary 
    - *Xw = Y*
    - minimise square error (MSE), or ERM using square error (emipirical risk minimization)
    - deterministic solution: $w = (X^{T}X)^{-1}X^{T}Y$
    - adding a bias term in *w* by inserting a row of 1s in *X*

- k-NN k-nearest neighbour
    - non-linear decision boundary
    - classification: f(x) = majority vote of $I_{x} = {i: x_{i} \in N(x;k)}$ N(x;k) is the set of k nearest training inputs
    - regression: $f(x) = \frac{1}{k} \sum_{i \in I_{x}}y_{i}$ ("*local mean*")
    - small k: overfitting, irregular decision boundary; large k: underfitting, smoothing decision boundary.
    - Bayes rule/estimator/clasifier for the K-class classification problem: $\argmax_{k} P(y=k|x)$; corresponding Bayes error rate/risk: $E_{x}[1-\max_{k}P(y=k|x)]$. 
    - [Asymptotic optimality of k-NN](https://www.cs.cmu.edu/~aarti/Class/10701/recitation/knn_asymp.pdf): the asymptotic expected risk of 1NN is no more than two times the Bayes risk.

- kernel methods




[back](../)
