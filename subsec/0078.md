---
layout: default
---
## Supplementary notes for COMP0078 Supervised Learning
- [Lecture 01 LR, k-NN, Bayes estimator, Bias-Variance trade-off, NFL theorem](https://github.com/YHJYH/Yuheng-Jia/blob/main/subsec/0078.md#lecture-01-lr-k-nn-bayes-estimator-bias-variance-trade-off-nfl-theorem)
- [Lecture 02]()

### Lecture 01 LR, k-NN, Bayes estimator, Bias-Variance trade-off, NFL theorem
- Linear regression
    - linear decision boundary 
    - *Xw = Y*
    - minimise square error (MSE), or ERM using square error (emipirical risk minimization)
    - deterministic solution: $w = (X^{T}X)^{-1}X^{T}Y$
    - adding a bias term in *w* by inserting a row of 1s in *X*

- k-NN k-nearest neighbour
    - non-linear decision boundary
    - classification: f(x) = majority vote of $I_{x} = {i: x_{i} \in N(x;k)}$ N(x;k) is the set of k nearest training inputs
    - regression: $f(x) = \frac{1}{k} \sum_{i \in I_{x}}y_{i}$ ("*local mean*")
    - small k: overfitting, irregular decision boundary; large k: underfitting, smoothing decision boundary.
    - Bayes rule/estimator/clasifier for the K-class classification problem: $\arg_{k}\max P(y=k|x)$; corresponding Bayes error rate/risk: $E_{x}[1-\max_{k}P(y=k|x)]$. 
    - [Asymptotic optimality of k-NN](https://www.cs.cmu.edu/~aarti/Class/10701/recitation/knn_asymp.pdf): the asymptotic expected risk of 1NN is no more than two times the Bayes risk. Lec01 P50.
    - [kNN from a Bayesian viewpoint](https://stats.stackexchange.com/a/157509): $P(y=k|x)=K_{k}/K$. Lec01 P49.

LR vs. k-NN
- 1 v 2
- 3 s s 

### Lecture 02 
- kernel methods




[back](../)
