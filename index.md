## A course on multi-armed bandits and reinforcement learning

**Course description**

In many sequential decision making problems, ranging from electronic commerce (recommendation systems, internet advertising, 
content optimization), revenue and inventory management to playing Atari games, the decision maker faces a fundamental tradeoff 
between taking the option that has performed best in the past and exploring new options to gather more information, i.e. 
the exploitation vs. exploration. Multi-armed bandits and reinforcement learning are classic paradigms of sequential decision making that 
capture this tradeoff. This course will discuss recent advances in algorithm design for these paradigms along with techniques 
for theoretical performance analysis.

**Lecture notes**

| Topic|  Lecture notes |
|------|-----------|
|Introduction | [Introduction slides](Intro.pdf) |
|The stochastic multi-armed bandit problem <br> lower bounds, greedy algorithm |[Lecture 2](Lecture%202.pdf) |
|UCB algorithm | [Lecture 3](Lecture%203.pdf) |
|Thompson Sampling | [Overview slides](Thompson%20Sampling%20overview.pdf), [Lecture 4](Lecture%204.pdf), [Lecture 5](Lecture%205.pdf)|
|Linear bandits (LinUCB) | [Lecture notes](Lecture%20linear%20bandit.pdf)|
|Introduction to MDP <br> Value iteration, Q-value iteration, Policy iteration | [Lecture notes](Lecture%20MDP.pdf)|
|RL algorithms, some theoretical guarantees | [Slides](RL%20guarantees%20SDM%20v2.pdf)|
